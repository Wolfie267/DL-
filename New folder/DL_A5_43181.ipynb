{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_GcJlVrDgBk"
      },
      "source": [
        "Name: **Pranit Zanwar**<br>\n",
        "Div: **BE09-S09**<br>\n",
        "Roll no: **43181**<br>\n",
        "Title: **Assignment 5: Implement the Continuous Bag of Words (CBOW) Model**<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V51q50EbF-T9"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "from keras.preprocessing import text\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import pad_sequences\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBUwYdBJElVz"
      },
      "outputs": [],
      "source": [
        "#taking random sentences as data\n",
        "data = \"\"\"Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. \n",
        "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and Transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\n",
        "\"\"\"\n",
        "dl_data = data.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "celNk9LmEvm8",
        "outputId": "4e2143b6-92dc-452f-f468-7c9e6238e287"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 75\n",
            "Vocabulary Sample: [('learning', 1), ('deep', 2), ('networks', 3), ('neural', 4), ('and', 5), ('as', 6), ('of', 7), ('machine', 8), ('supervised', 9), ('have', 10)]\n"
          ]
        }
      ],
      "source": [
        "#tokenization\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(dl_data)\n",
        "word2id = tokenizer.word_index\n",
        "\n",
        "word2id['PAD'] = 0\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in dl_data]\n",
        "\n",
        "vocab_size = len(word2id)\n",
        "embed_size = 100\n",
        "window_size = 2 \n",
        "\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Vocabulary Sample:', list(word2id.items())[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAxNYDanInQC"
      },
      "outputs": [],
      "source": [
        "#generating (context word, target/label word) pairs\n",
        "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
        "    context_length = window_size*2\n",
        "    for words in corpus:\n",
        "        sentence_length = len(words)\n",
        "        for index, word in enumerate(words):\n",
        "            context_words = []\n",
        "            label_word   = []            \n",
        "            start = index - window_size\n",
        "            end = index + window_size + 1\n",
        "            \n",
        "            context_words.append([words[i] \n",
        "                                 for i in range(start, end) \n",
        "                                 if 0 <= i < sentence_length \n",
        "                                 and i != index])\n",
        "            label_word.append(word)\n",
        "\n",
        "            x = pad_sequences(context_words, maxlen=context_length)\n",
        "            y = np_utils.to_categorical(label_word, vocab_size)\n",
        "            yield (x, y)\n",
        "            \n",
        "i = 0\n",
        "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
        "    if 0 not in x[0]:\n",
        "        # print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
        "    \n",
        "        if i == 10:\n",
        "            break\n",
        "        i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rb5dNmoZKZBv",
        "outputId": "b859c07e-6989-420d-b169-8aa0b93ff367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 4, 100)            7500      \n",
            "                                                                 \n",
            " lambda (Lambda)             (None, 100)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 75)                7575      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,075\n",
            "Trainable params: 15,075\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "#model building\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "\n",
        "cbow = Sequential()\n",
        "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
        "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
        "cbow.add(Dense(vocab_size, activation='softmax'))\n",
        "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "print(cbow.summary())\n",
        "\n",
        "# from IPython.display import SVG\n",
        "# from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "# SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, rankdir='TB').create(prog='dot', format='svg'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xs12C3MDK1q4",
        "outputId": "fe8783b3-0ee1-4286-be40-6713afa14f9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tLoss: 433.88288593292236\n",
            "\n",
            "Epoch: 2 \tLoss: 428.80262994766235\n",
            "\n",
            "Epoch: 3 \tLoss: 425.27664613723755\n",
            "\n",
            "Epoch: 4 \tLoss: 422.0698399543762\n",
            "\n",
            "Epoch: 5 \tLoss: 419.70696926116943\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, 6):\n",
        "    loss = 0.\n",
        "    i = 0\n",
        "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
        "        i += 1\n",
        "        loss += cbow.train_on_batch(x, y)\n",
        "        if i % 100000 == 0:\n",
        "            print('Processed {} (context, word) pairs'.format(i))\n",
        "\n",
        "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "TZ3_QGKVK6Tj",
        "outputId": "cd9d167a-85a3-4cc9-eccb-faf997526122"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(74, 100)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>deep</th>\n",
              "      <td>-0.023805</td>\n",
              "      <td>0.014639</td>\n",
              "      <td>0.007089</td>\n",
              "      <td>0.012630</td>\n",
              "      <td>-0.019672</td>\n",
              "      <td>-0.024277</td>\n",
              "      <td>0.032819</td>\n",
              "      <td>0.036645</td>\n",
              "      <td>-0.017602</td>\n",
              "      <td>0.041846</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.014910</td>\n",
              "      <td>0.031716</td>\n",
              "      <td>0.051072</td>\n",
              "      <td>-0.014720</td>\n",
              "      <td>-0.004336</td>\n",
              "      <td>0.041430</td>\n",
              "      <td>0.008264</td>\n",
              "      <td>0.029062</td>\n",
              "      <td>0.036095</td>\n",
              "      <td>-0.054683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>networks</th>\n",
              "      <td>0.037381</td>\n",
              "      <td>0.012409</td>\n",
              "      <td>-0.048280</td>\n",
              "      <td>0.010494</td>\n",
              "      <td>0.014020</td>\n",
              "      <td>-0.050136</td>\n",
              "      <td>-0.056002</td>\n",
              "      <td>-0.016235</td>\n",
              "      <td>-0.024859</td>\n",
              "      <td>0.037838</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.040886</td>\n",
              "      <td>-0.012996</td>\n",
              "      <td>0.033726</td>\n",
              "      <td>0.033758</td>\n",
              "      <td>0.022307</td>\n",
              "      <td>-0.026201</td>\n",
              "      <td>0.051270</td>\n",
              "      <td>0.039335</td>\n",
              "      <td>0.028746</td>\n",
              "      <td>0.046762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neural</th>\n",
              "      <td>0.010798</td>\n",
              "      <td>-0.013446</td>\n",
              "      <td>-0.036526</td>\n",
              "      <td>0.040533</td>\n",
              "      <td>-0.028618</td>\n",
              "      <td>-0.038640</td>\n",
              "      <td>0.031019</td>\n",
              "      <td>-0.022227</td>\n",
              "      <td>-0.006474</td>\n",
              "      <td>0.003521</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.034832</td>\n",
              "      <td>-0.004115</td>\n",
              "      <td>0.032460</td>\n",
              "      <td>0.006391</td>\n",
              "      <td>-0.029102</td>\n",
              "      <td>-0.031513</td>\n",
              "      <td>-0.049257</td>\n",
              "      <td>0.025712</td>\n",
              "      <td>0.034823</td>\n",
              "      <td>0.029035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>-0.042606</td>\n",
              "      <td>-0.042891</td>\n",
              "      <td>0.007063</td>\n",
              "      <td>-0.033965</td>\n",
              "      <td>-0.027141</td>\n",
              "      <td>0.030406</td>\n",
              "      <td>-0.049688</td>\n",
              "      <td>0.049656</td>\n",
              "      <td>0.049267</td>\n",
              "      <td>-0.017985</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.025883</td>\n",
              "      <td>0.040344</td>\n",
              "      <td>0.036805</td>\n",
              "      <td>-0.026118</td>\n",
              "      <td>-0.035882</td>\n",
              "      <td>0.018707</td>\n",
              "      <td>0.043800</td>\n",
              "      <td>0.008576</td>\n",
              "      <td>0.001721</td>\n",
              "      <td>0.037442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>as</th>\n",
              "      <td>-0.010727</td>\n",
              "      <td>-0.016468</td>\n",
              "      <td>-0.048425</td>\n",
              "      <td>-0.025175</td>\n",
              "      <td>-0.020269</td>\n",
              "      <td>0.023518</td>\n",
              "      <td>-0.036965</td>\n",
              "      <td>-0.029299</td>\n",
              "      <td>-0.023431</td>\n",
              "      <td>-0.028872</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.019888</td>\n",
              "      <td>0.001340</td>\n",
              "      <td>0.049596</td>\n",
              "      <td>-0.031154</td>\n",
              "      <td>-0.023594</td>\n",
              "      <td>-0.025513</td>\n",
              "      <td>-0.013557</td>\n",
              "      <td>0.037622</td>\n",
              "      <td>-0.021166</td>\n",
              "      <td>0.027056</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                0         1         2         3         4         5   \\\n",
              "deep     -0.023805  0.014639  0.007089  0.012630 -0.019672 -0.024277   \n",
              "networks  0.037381  0.012409 -0.048280  0.010494  0.014020 -0.050136   \n",
              "neural    0.010798 -0.013446 -0.036526  0.040533 -0.028618 -0.038640   \n",
              "and      -0.042606 -0.042891  0.007063 -0.033965 -0.027141  0.030406   \n",
              "as       -0.010727 -0.016468 -0.048425 -0.025175 -0.020269  0.023518   \n",
              "\n",
              "                6         7         8         9   ...        90        91  \\\n",
              "deep      0.032819  0.036645 -0.017602  0.041846  ... -0.014910  0.031716   \n",
              "networks -0.056002 -0.016235 -0.024859  0.037838  ... -0.040886 -0.012996   \n",
              "neural    0.031019 -0.022227 -0.006474  0.003521  ... -0.034832 -0.004115   \n",
              "and      -0.049688  0.049656  0.049267 -0.017985  ... -0.025883  0.040344   \n",
              "as       -0.036965 -0.029299 -0.023431 -0.028872  ... -0.019888  0.001340   \n",
              "\n",
              "                92        93        94        95        96        97  \\\n",
              "deep      0.051072 -0.014720 -0.004336  0.041430  0.008264  0.029062   \n",
              "networks  0.033726  0.033758  0.022307 -0.026201  0.051270  0.039335   \n",
              "neural    0.032460  0.006391 -0.029102 -0.031513 -0.049257  0.025712   \n",
              "and       0.036805 -0.026118 -0.035882  0.018707  0.043800  0.008576   \n",
              "as        0.049596 -0.031154 -0.023594 -0.025513 -0.013557  0.037622   \n",
              "\n",
              "                98        99  \n",
              "deep      0.036095 -0.054683  \n",
              "networks  0.028746  0.046762  \n",
              "neural    0.034823  0.029035  \n",
              "and       0.001721  0.037442  \n",
              "as       -0.021166  0.027056  \n",
              "\n",
              "[5 rows x 100 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weights = cbow.get_weights()[0]\n",
        "weights = weights[1:]\n",
        "print(weights.shape)\n",
        "\n",
        "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFs2IAn_LAYS",
        "outputId": "87ae1249-6a80-484f-b4a2-4d5f20734db7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(74, 74)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'deep': ['analysis',\n",
              "  'recognition',\n",
              "  'representation',\n",
              "  'belief',\n",
              "  'unsupervised']}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "distance_matrix = euclidean_distances(weights)\n",
        "print(distance_matrix.shape)\n",
        "\n",
        "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
        "                   for search_term in ['deep']}\n",
        "\n",
        "similar_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgGYGy1Q11Tu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZO7IyhK11Tv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}